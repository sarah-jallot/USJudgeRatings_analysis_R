---
title: "Statistics - Homework 1"
author: "Victoire"
date: "11/3/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\textbf{Problem 1: Estimating parameters of a Poisson distribution to model the number of goals scored in football}

\textbf{Question 1}

The Poisson law is a discrete distribution as $D_X=N\star$


The Poisson law is used to describe rare events in a large population. 

Ex 1: the Seine overflooding x times over 100 years

Ex 2: the probabiliy that a restaurant stays empty for a whole night although it normally serves around 1000 people each night 

Ex 3: the probability that a glass breaks among a bunch of thick glasses


\textbf{Question 2}


We know that the Poisson law admits an expectation and a variance, as it is a usal law.
Let X follow a Poisson law of parameter $\lambda$. 

$E(X)=\sum_{k=0}^{\infty}\frac{e^{-\lambda}\lambda^kk}{k!}$

$E(X)={e^{-\lambda}}\sum_{k=1}^{\infty}\frac{\lambda^kk}{k!}$

$E(X)={e^{-\lambda}}\sum_{k=1}^{\infty}\frac{\lambda^k}{(k-1)!}$ as the term in 0 is null

$E(X)={e^{-\lambda}}\sum_{k=0}^{\infty}\frac{\lambda^{k+1}}{k!}$

$E(X)={{\lambda}e^{-\lambda}}\sum_{k=0}^{\infty}\frac{\lambda^k}{k!}$

We recognize an exponential series of parameter $\lambda$>0, and thus:
E(X) = $\lambda$

X admits a variance as:
$E(X^2)=\sum_{k=0}^{\infty}\frac{e^{-\lambda}\lambda^kk^2}{k!}$

and we can see that:$k^2=k(k-1)+k$

$E(X^2)=\sum_{k=0}^{\infty}\frac{e^{-\lambda}\lambda^kk(k-1)}{k!}+E(X)$

$E(X^2)=\sum_{k=2}^{\infty}\frac{e^{-\lambda}\lambda^kk(k-1)}{k!}+\lambda$ as the 2 first terms are null

$E(X^2)=\sum_{k=2}^{\infty}\frac{e^{-\lambda}\lambda^k}{(k-2)!}+\lambda$

$E(X^2)=\lambda^2e^{-\lambda}\sum_{k=0}^{n}\frac{\lambda^k}{k!}+\lambda$

We recognize an exponential series of parameter $\lambda$>0, and thus:
$E(X^2)=\lambda^2+\lambda$

We can now compute the variance: 
$V(X)=E(X^2)-[E(X)]^2)$

$V(X)=\lambda^2+\lambda-\lambda^2$

$V(X)=\lambda$


\textbf{Question 3 - We are provided with n independent observations of a Poisson random variable of parameter $\sigma$.}

We have n independent observations n independent observations of a Poisson random variable of parameter $\sigma$.
For $i\in[1,n]$, $X_i$ corresponds to the number of goals scored during the ith observed football match.
Our observations follow a Poisson law of unknown parameter $\theta$ that we want to estimate.


Write the corresponding statistical model.

M={P(X=k)=$e^{-\theta}\frac{\theta^k}{k!},k\in{N},k>0,\theta\in{R}\star+$}


\textbf{Question 4}


Let's call l the likelihood function of our model.

$l(x_1,...,x_n|\theta)=\prod_{i=1}^{n}l(x_i|\theta)$ by independence of the observations

$l(x_1,...,x_n|\theta)=\prod_{i=1}^{n}P(X_i=x_i|\theta)$

$l(x_1,...,x_n|\theta)=\prod_{i=1}^{n}\frac{e^{-\theta}\theta^{x_i}}{x_i!}$


Let's the Maximum Likelihood Estimator ${\hat{\sigma}}_{ML}$.

To do so, let's compute the log likelihood function of our model, called L.

$L(x_1,...,x_n|\theta)=\log(l(x_1,...,x_n|\theta))$

$L(x_1,...,x_n|\theta)=\log(\prod_{i=1}^{n}\frac{e^{-\theta}\theta^{x_i}}{x_i!})$

$L(x_1,...,x_n|\theta)=\sum_{i=1}^{n}\log(\frac{e^{-\theta}\theta^{x_i}}{x_i!})$

$L(x_1,...,x_n|\theta)=-n*\theta+\sum_{i=1}^{n}x_i*\log(\theta)-\sum_{i=1}^{n}\log(x_i!)$

Finding the maximum of the likelihood function is equivalent to finding the maximum of the log likelihood function.

If we are at themaximum, then $L'(x1,...,x_n|\theta)=0$

$\forall\theta\in{R},\theta>0$

$L'(x1,...,x_n|\theta)=-n+\sum_{i=1}^{n}\frac{X_i}{\theta}$

Thus,

$L'(x1,...,x_n|\theta)=0\iff\theta=\frac{1}{n}{\sum_{i=1}^{n}x_i}$

The Maximum Likelihood Estimator is :

${\hat{\theta}}_{ML}=\frac{1}{n}{\sum_{i=1}^{n}x_i}$

\textbf{Question 5 - Prove that $\sqrt{n}(\theta_{ML}-\theta)$ converge in distribution when $n\to\infty$}

Let's compute the expectation of $\theta_{ML}$

$E({\hat{\theta}}_{ML})=E(\frac{1}{n}\sum_{i=1}^{n}X_i)$

$E({\hat{\theta}}_{ML})=\frac{1}{n}\sum_{i=1}^{n}E(X_i)$

$E({\hat{\theta}}_{ML})=\theta$

Thus, ${\hat{\theta}}_{ML}$ is an unbiased estimator of $\theta$. 

$\sqrt{n}({\hat{\theta}}_{ML}-\theta)=\sqrt{n}(\frac{\sum_{i=1}^{n}X_i}{n}-E(X))$ as all $X_i$ are idd and thus follow the same law as X ~ P($\theta$)

According to the Central limit theorem, $\sqrt{n}(\hat\theta_{ML}-\theta)$ converges in distribution when $n\to\infty$ to the normal distribution N(0, $\theta$)

\textbf{Question 6}

According to question 5, $\sqrt{n}(\hat\theta_{ML}-\theta)$ converges in distribution when $n\to\infty$ to the normal distribution N(0, $\theta$).

Also, $\hat\theta_{ML}$ congerves in probability when $n\to\infty$  to $\theta$, due to its construction. 

Using the continuous mapping theorem, with the function $g:x\to\frac{1}{\sqrt{x}}$, we obtain that :
$\frac{1}{\sqrt{\hat\theta_{ML}}}$ converges in probability when $n\to\infty$  to $\frac{1}{\sqrt{\theta}}$

Using the Slutsky lemma, we obtain that:
$\frac{\sqrt{n}(\hat\theta_{ML}-\theta)}{\sqrt{\hat\theta_{ML}}}$ converges in distribution when $n\to\infty$ to the normal distribution N(0,1). 

Using R: 
```{r}
par(mfrow=c(1,3))

# Buiding our distribution for our variable 

Nattempts = 1000
nsample   = 100

theta = 3
set.seed(43) 
Nattempts = 1000
nsample = 100
variable_sample = c()

for (i in 1:Nattempts)  # can be written without the for loop (nicer) !
{
  pois_sample = rpois(nsample, theta)
  variable_sample[i] = (mean(pois_sample) - theta ) * sqrt(nsample) / sqrt(theta)
}  
hist(variable_sample, prob=TRUE)
d = density(variable_sample)
lines(d, col='red')

# Building the histogram of the normal distribution function
 
normal_distrib = rnorm(Nattempts, mean = 0, sd = 1)
hist(normal_distrib, prob=TRUE)
d = density(normal_distrib)
lines(d, col='red')

# Building our qq plot 

qqnorm(variable_sample, xlab= 'Quantiles for our variable', ylab='Quantiles for a N(0,1)')
```
\textbf{Question 7}

First, the variance is unknown and therefore we estimate it using the empirical variance that will call $S_n^2$. 

$S_n^2=\frac{1}{n-1}\sum_{1}^{n}(X_i-\bar{X_n})^2$

$\bar{X_n}$ is asymptotically gaussian.

$\bar{X_n}$ and $S_n^2$ are independent according to the Student's theorem.

Let $\hat\sigma=\sqrt{S_n^2}$

We call $t_{1-\alpha/2}$ the quantile at degree $1-\alpha$ for the Student's law.

$IC(m)=[\frac{\sum_{i=1}^{n}x_i}{n}-t_{1-\alpha/2}\frac{\hat{\sigma}}{\sqrt(n)};\frac{\sum_{i=1}^{n}x_i}{n}+t_{1-\alpha/2}\frac{\hat{\sigma}}{\sqrt(n)}]$ is an asymptotical confidence interval for $\theta$ at the level $1-\alpha$. 

As we want an interval that is at least at the level $1-\alpha$, we can choose $\epsilon>0$, such that our new interval is:
$IC(m)=[\frac{\sum_{i=1}^{n}x_i}{n}-t_{1-\alpha/2}\frac{\hat{\sigma}}{\sqrt(n)}-\epsilon;\frac{\sum_{i=1}^{n}x_i}{n}+t_{1-\alpha/2}\frac{\hat{\sigma}}{\sqrt(n)}+\epsilon]$

And we so we obtain: $P(\theta\in[\frac{\sum_{i=1}^{n}x_i}{n}-t_{1-\alpha/2}\frac{\hat{\sigma}}{\sqrt(n)}-\epsilon;\frac{\sum_{i=1}^{n}x_i}{n}+t_{1-\alpha/2}\frac{\hat{\sigma}}{\sqrt(n)}+\epsilon])\ge1-\alpha$

And as we don't need to find the estimated variance because it equals the estimated mean, we obtain that: 
and it is a normal distribution so put q

\textbf{Question 8}

We use the $\delta$-method with $g:x\to2\sqrt{x}$

Then, $\forall{x}>0$, $g'(x)=\frac{1}{\sqrt{x}}$

We apply the $\delta$-method using this function g to $\sqrt{n}(\theta_{ML}-\theta)$

We know that :

$E(\hat\theta_{ML})=\theta$ 

we compute :

$[g'(\theta)]^2V(\hat\theta_{ML})=1$

We obtain : $\sqrt{n}(2\sqrt{\hat\theta_{ML}}-2\sqrt{\theta})$ converges in distribution when $n\to\infty$ to the normal distribution N(0,1). 

\textbf{Question 9}

\textbf{Question 10}

We know that if X follows a Poisson lax of parameter $\lambda$, then E(X)=V(X)=$\lambda$. 

Therefore, we could choose the following estimators, based on obervation:

$\theta_1=\frac{1}{n}\sum_{1}^{n}X_i$ (the empirical expectation / mean) using the first moment of a Poisson distribution.

$\theta_2=\frac{1}{n-1}\sum_{1}^{n}(X_i-\bar{X_n})^2$ (the empirical variance) using the first and second moments of a Poisson distribution.

Note: we could also propose $\theta_2=\frac{1}{n}\sum_{1}^{n}(X_i-\bar{X_n})^2$.

$\theta_1$ is the MLE we have already studied.

\textbf{Question 11}


Compute the bias of $\hat{\theta}_{ML}$

$b_{\theta}=E({\hat{\theta}}_{ML})-\theta$

$b_{\theta}=0$ and thus ${\hat{\theta}}_{ML}$ is an unbiased estimator of $\theta$

Compute the variance of ${\hat{\theta}}_{ML}$: 

$Var({\hat{\theta}}_{ML})=Var(\frac{\sum_{i=1}^{n}X_i}{n})$

$Var({\hat{\theta}}_{ML})=\frac{\sum_{i=1}^{n}Var(X_i)}{n^2}$ as the $X_i$ are idependent variables 

$Var({\hat{\theta}}_{ML})=\frac{\theta}{n}$ as the $X = i$ follow Poisson distribution of parameter $\theta$

Compute the quadratic risk of ${\hat{\theta}}_{ML}$: 

$Q_{\theta}=b_{\theta}^2+Var({\hat{\theta}}_{ML})$

$Q_{\theta}=\frac{\theta}{n}$

\textbf{Question 12: compute the Cramer Rao bound}

Faire le contraire car I = V et pas 1/I = V

$I_{n\theta\ast}=Var\ast(L'(x1,...,x_n|\theta\ast))$

$I_{n\theta\ast}=Var\ast(-n+\sum_{i=1}^{n}\frac{X_i}{\theta\ast})$

$I_{n\theta\ast}=\sum_{i=1}^{n}\frac{Var\ast(X_i)}{{\theta\ast}^2}$ as the $X_i$ are independent 

$I_{n\theta\ast}=\frac{n}{\theta\ast}$

As $\frac{1}{I_{n\theta\ast}}={Var\ast}(\hat\theta_{ML})$, the MLE is efficient. Thus, it is the best estimator of $\theta$ among all linear and unbiased estimator of $\theta$. 

\textbf{Question 13}

$\hat\theta_2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar{X_n})^2$

$\hat\theta_2=\frac{1}{n}\sum_{i=1}^{n}((X_i-\theta)-(\bar{X_n}-\theta))^2$

$\hat\theta_2=\frac{1}{n}\sum_{i=1}^{n}[(X_i-\theta)^2+(\bar{X_n}-\theta)^2-2(X_i-\theta)(\bar{X_n}-\theta)]$

$\hat\theta_2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2+(\bar{X_n}-\theta)^2-2\frac{1}{n}(\bar{X_n}-\theta)\sum_{i=1}^{n}(X_i-\theta)$

$\hat\theta_2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2+(\bar{X_n}-\theta)^2-2(\bar{X_n}-\theta)(\frac{1}{n}\sum_{i=1}^{n}X_i-\theta)$

$\hat\theta_2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2+(\bar{X_n}-\theta)^2-2(\bar{X_n}-\theta)(\bar{X_n}-\theta)$

$\hat\theta_2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2-(\bar{X_n}-\theta)^2$

\textbf{Question 14: compute $E((\bar{X_n}-\theta)^2)$}

$E((\bar{X_n}-\theta)^2)=E(\bar{X_n}^2+\theta^2-2\theta\bar{X_n})$

$E((\bar{X_n}-\theta)^2)=E(\bar{X_n}^2)+\theta^2-2\theta*E(\bar{X_n})$ due to the linearity of the expectation

$E((\bar{X_n}-\theta)^2)=Var(\bar{X_n})+E(\bar{X_n})^2+\theta^2-2\theta*E(\bar{X_n})$

$E((\bar{X_n}-\theta)^2)=\frac{\theta}{n}+\theta^2+\theta^2-2\theta^2$

$E((\bar{X_n}-\theta)^2)=\frac{\theta}{n}$

Prove that $\hat\theta_2$ is a biased estimator of $\theta$:

$E(\hat\theta_2)-\theta=E(\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2-(\bar{X_n}-\theta)^2)-\theta$ due to question 13 

$E(\hat\theta_2)-\theta=\frac{1}{n}\sum_{i=1}^{n}E((X_i-\theta)^2)-E((\bar{X_n}-\theta)^2)-\theta$ by linearity of the expectation

$E((X_i-\theta)^2)=V(X_i-\theta)+[E(X_i-\theta)]^2$

$E((X_i-\theta)^2)=V(X_i)$

$E((X_i-\theta)^2)=\theta$ 


Thus,

$E(\hat\theta_2)-\theta=\theta-E((\bar{X_n}-\theta)^2)-\theta$ 

$E(\hat\theta_2)-\theta=-E((\bar{X_n}-\theta)^2)$ 

$E(\hat\theta_2)-\theta=-V(\bar{X_n}-\theta)-[E(\bar{X_n}-\theta)]^2$ 

$E(\hat\theta_2)-\theta=-V(\bar{X_n})$ as $\bar{X_n}$ is an unbiased estimator of $\theta$

$E(\hat\theta_2)-\theta=-\frac{\theta}{n}$ and thus $\hat\theta_2$ is a biased estimator of $\theta$. 

To get an unbiased estimator of $\theta$, we need the bias to be equal to 0.

$E(\hat\theta_2)-\theta=-\frac{\theta}{n}\iff1E(\hat\theta_2)=\theta-\frac{\theta}{n}$

$E(\hat\theta_2)-\theta=-\frac{\theta}{n}\iff1E(\hat\theta_2)=\frac{(n-1)\theta}{n}$

$E(\hat\theta_2)-\theta=-\frac{\theta}{n}\iff1E(\frac{n\hat\theta_2}{n-1})=\theta$

Then, we can use $\frac{(n)\hat\theta_2}{n-1}$ to get an unbiased estimator of $\theta$.

\textbf{Question 15}

$\hat\theta_2=\frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X_n})^2$

According to question 13, we can rewrite $\hat\theta_2$ as follows: $\hat\theta_2=\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2-(\bar{X_n}-\theta)^2$.

Then, 

$\sqrt{n}(\hat\theta_2-\theta)=\sqrt{n}[\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2-\theta-(\bar{X_n}-\theta)^2]$

$\sqrt{n}(\hat\theta_2-\theta)=\sqrt{n}[\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2-\theta]-\sqrt{n}[(\bar{X_n}-\theta)^2]$

We will look at the convergence of the two expressions into brackets separately at first.

According to previous calculation in question 13, we know that:

$E(\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2)=\theta$

Let's write $Y_i=(X_i-\theta)^2$, we know that the $Y_i$ are idd and follow the same law as a variable that we will call Y, as linear combinations of the variables $X_i$ that are iid.

We have $E(\frac{1}{n}\sum_{i=1}^{n}Y_i)=\theta$

According to the Central Limit Theorem, $\sqrt{n}(\frac{1}{n}\sum_{i=1}^{n}Y_i-\theta)$ converges in distribution when $n\to\infty$ to the law N(0, V(Y)).

$V(Y)=V[(X-\theta)^2]=2\theta^2+\theta$ according to the hint

Thus, $\sqrt{n}(\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2)-\theta)$ converges in distribution when $n\to\infty$ to the law N(0,$2\theta^2+\theta$).

Then, we have to study the convergence of $\sqrt{n}[(\bar{X_n}-\theta)^2]$

According to question 14, $E((\bar{X_n}-\theta)^2)=\frac{\theta}{n}$

Thus, $E(\sqrt{n}(\bar{X_n}-\theta)^2)=\frac{\theta}{\sqrt{n}}$

Let $\epsilon>0$. 

According to Markov inequality, $P(\sqrt{n}(\bar{X_n}-\theta)^2>\epsilon)\le\frac{\theta}{\sqrt{n}\epsilon}$

Thus, $lim_{n\to\infty}P(\sqrt{n}(\bar{X_n}-\theta)^2>\epsilon)=0$

We can conclude that $\sqrt{n}(\bar{X_n}-\theta)^2$ converges in probability when ${n\to\infty}$ to 0.

Finally, using the Slutsky lemma, we can conclude that $\sqrt{n}[\frac{1}{n}\sum_{i=1}^{n}(X_i-\theta)^2-\theta]-\sqrt{n}[(\bar{X_n}-\theta)^2]$ converges in distribution when ${n\to\infty}$ to N(0,$\theta^2+\theta$).

Thus, $\sqrt{n}(\hat\theta_2-\theta)$ converges in distribution when ${n\to\infty}$ to N(0,$2\theta^2+\theta$).

\textbf{Question 16}

$G_X(s)=E(e^{sX})$

$G_X(s)=\sum_{k=0}^{\infty}\frac{\theta^ke^{-\theta}e^{sk}}{k!}$

$G_X(s)=e^{-\theta}\sum_{k=0}^{\infty}\frac{(e^s\theta)^k}{k!}$ we recognize an exponential series with term $e^s\theta>0$.

$G_X(s)=e^{-\theta}e^{\theta*e^s}$

$G_X(s)=e^{\theta*(e^s-1)}$

Recover the result of question 2 i.e. if $X_1$~P($\lambda_1$) and $X_2$~P($\lambda_2$), and the two variables are independent, then $X_1+X_2$~P($\lambda_1+\lambda_2$). 

$G_{X_1+X_2}(s)=E(e^{s(X_1+X_2)})$

$G_{X_1+X_2}(s)=E(e^{sX_1}e^{sX_2})$

$G_{X_1+X_2}(s)=E(e^{sX_1})E(e^{sX_2})$ by independence of the two variables 

$G_{X_1+X_2}(s)=e^{\theta_1*(e^s-1)}e^{\theta_2*(e^s-1)}$ according to previous result 

$G_{X_1+X_2}(s)=e^{(\theta_1+\theta_2)*(e^s-1)}$ and therefore, by uniquness of the generating function, ${X_1+X_2}$ follows a Poisson distribution of parameter $\theta_1+\theta_2$. 

Prove that $Var((X_i-\theta)^2)=2\theta^2+\theta$

To do so, let's prove first that $\Rightarrow{}G^{(i)}_X(s)=\sum_{k=0}^{\infty}\frac{\theta^ke^{-\theta}k^ie^{sk}}{k!}$ and   $\forall{i}\in{R\star},{}G_X(0)=E(X^i)$

For k=1, $G_X(s)=\sum_{k=0}^{\infty}\frac{\theta^ke^{-\theta}e^{sk}}{k!}$

$\Rightarrow{}G_X(s)=\sum_{k=0}^{\infty}\frac{\theta^ke^{-\theta}e^{sk}}{k!}$

$\Rightarrow{}G'_X(s)=\sum_{k=0}^{\infty}\frac{\theta^ke^{-\theta}ke^{sk}}{k!}$

$\Rightarrow{}G'_X(0)=\sum_{k=0}^{\infty}\frac{\theta^ke^{-\theta}}{k!}$ 

$\Rightarrow{}G^{(1)}_X(0)=E(X^1)$

Let us consider that it is true for one i in $N\star$. Let's prove that it is also true for i+1. 

$G^{(i+1)}_X(s)=G^{(i)}{'}_X(s)$

$G^{(i+1)}_X(s)=\sum_{k=0}^{\infty}\frac{\theta^ke^{-\theta}k^{i+1}e^{sk}}{k!}$ by recurrence hypothesis 

$G^{(i+1)}_X(0)=\sum_{k=0}^{\infty}\frac{\theta^ke^{-\theta}k^{i+1}}{k!}$ 

$G^{(i+1)}_X(0)=E(X^{i+1})$ which ends the recurrence.




\textbf{Problem 2: Analysis of the USJudgeRatings data set}

\textbf{I. Introduction}

In this section of our report, the main idea is to discover the data, understand the features and get a first idea of the values they tend to take.

\textbf{1. Basic information on the data}
```{r}
# loading the dataset, that is already in R
data(USJudgeRatings)
```
Visualizing the first lines/obrservations of our dataframe:
```{r}
library(knitr)
head(kable(USJudgeRatings))
```

```{r}
# getting first information on the data 
help(USJudgeRatings)
```
Using the help function, we learn that the data is about Lawyers' Ratings of State Judges in the US Superior Court. We can note that the data is quite old, as the source is provided: New Haven Register, 14 January, 1977 (from John Hartigan). 

The data frame contains 43 observations on 12 numeric variables. This means that we have the rating for 43 State Judges in the US Superior Court. The criteria to evaluate the States corresponds to the features:  

[,1]	CONT	Number of contacts of lawyer with judge.

[,2]	INTG	Judicial integrity.

[,3]	DMNR	Demeanor. (comportement)

[,4]	DILG	Diligence. (assiduit√©)

[,5]	CFMG	Case flow managing.

[,6]	DECI	Prompt decisions.

[,7]	PREP	Preparation for trial.

[,8]	FAMI	Familiarity with law.

[,9]	ORAL	Sound oral rulings.

[,10]	WRIT	Sound written rulings.

[,11]	PHYS	Physical ability.

[,12]	RTEN	Worthy of retention.

 We have to verify that we don't have any missing values in our dataframe:
```{r}
print(paste("there is", sum(is.na(USJudgeRatings)), "missing values in the dataframe"))
```

\textbf{2. A general description of the data}

To get a first idea of the data, we can compute the mean, the median and the variance for each variable:

First, we have to look at the nature of each variable to ensure that they are all numerical:

```{r}
str(USJudgeRatings, give.attr = FALSE)
```



Then, we obtain the following means and other information by variable:
```{r}
kable(summary(USJudgeRatings), format = 'markdown')
```

We observe that the means of the grades are very close for all variables, between 7 and 8. The medians are also in this area, but a bit higher that the means. Looking at the maximal and minimal values taken by the variables could also give us an idea of the attribution of the grades. We observe that the grade are mainly between 4 and 10.

We can note that it can be suprising at first to obtain a score below 10 for the variable CONT, but is explained by the fact that this variable is not a grade but the number of contacts of the lawyer with the judge. If it is a grade based on this number, 10 might be an outlier as grades are not supposed to be superior to 10.

The variables all have variances close to 1, which means that they are not too far from being reduced. 


Displaying the boxplots of each variable can help use visualize these results:
```{r}
boxplot(USJudgeRatings, cex.axis=0.5, main="Boxplot for the USJudgeRatings dataset", fig.align="center", out.width="50%")
```
This boxplot confirms our first impressions on the data.

\textbf{II. Analysis of the data}

in this section of our report, the main idea is to deduce a probabilistic setting from our obervations, and to study the link between variables. Due to the correlation matrix displayed below, we have decided to show first that the variables are generally very correlated, and then to study themn independently. This decision comes from the fact that it is quite curious to obtain such strong correlations among our variables, as it means that the same information is repeated through different columns. 

\textbf{1. General analysis of the dataset}

As just stated, we will look at the relationships among variables. A way to do this would be to plot the correlation matrix of the dataset, and to look at the coeeficients. To gain efficiency, we decided to plot a correlation heatmap, as it is viasually easier to read. 
```{r}
library(corrplot) 
par(mar=c(5,5,5,5)) 
Cor = round(cor(USJudgeRatings),2)
corrplot(Cor, method="ellipse")
```

Thanks to this correlation heatmap, we observe that variables are all highly positively correlated, except for the CONT variable that is almost not correlated to the others. Indeed, we can state so due to the color of the ellipse, but also due to their shapes. The more two variables are correlated, the thiner is the ellipse.

Note: this means that the criteria used to grade State Judges are not independent. Thus, some might be redundant. 

For instance, looking at the correlation heatmap, we can easily see that the PREP feature is highly correlated to all are features except from CONT. In the case of a regression, we might have deleted this column as the information seems to be redudant with all other features.

Another way analyze the link between variables is to plot the data two by two:

```{r}
library(GGally)
theme_set(theme_classic(base_size = 0.5))
ggpairs(USJudgeRatings)
```
The variables are highly correlated if their empirical values are correlated, in a way that the plot between two variables tends to draw a line. 

On these graphs, we can see that the variable CONT, i.e. the number of contacts of lawyer with judge, is the variable that seems to be the least correlated to others. It is encouraging for the impartiality of the grades if it means that the number of contacts between a lawyer and a judge does not influence the lawyer's ability to grade the judge impartially.

INTG, DMNR and PHYS would come then in the list. 

We recover the results that we deduced from the correlation heatmap. 

```{r}
#looking at the skewness and kurtosis of our variables : find a way to calculate them all at once 
library(e1071)
skewness(USJudgeRatings$CONT)
kurtosis(USJudgeRatings$CONT)
```

\textbf{2. Descriptive statistics}

In this part, we will look more closely at the information we can extract from the dataset. After studing variable one by one, we will compare them, focusing on the difference between highly correlated and less correlated variables. 

To show what it looks like when two variables are almost not correlated, we will use the variable CONT and then pick another one, here INTG for instance. 

a. Univariate analysis 

First, we will look for information on the variables one by one. 

For instance, we can have the following analysis for the first variable, CONT:

```{r}
par(mfrow=c(1,3))
hist(USJudgeRatings$CONT, prob=TRUE)
d = density(USJudgeRatings$CONT)
lines(d, col='red')

norm_distrib_CONT=rnorm(43,mean(USJudgeRatings$CONT))

hist(norm_distrib_CONT, prob=TRUE)
d = density(norm_distrib_CONT)
lines(d, col='red')

qqnorm(USJudgeRatings$CONT)
qqline(USJudgeRatings$CONT)
```
According to the histogram, the shape of a density could make us think of a normal distribution. Thus, we compare the quantiles to the ones of a normal law, and we remark that their is a linear relationship between the two variables. 
Note: as we have only 43 points, it might not be a large enough sample to get a very good interpretation of the law of CONT.

Note for us: find the coefficients of the regression line. 

Then, as the other variables are highly correlated, we can expect them to follow the same type of law. 
We can try for instance to compare the law of DILG and PREP.


```{r}
hist(USJudgeRatings$DILG, col=rgb(1,0,0,1/4), xlim=c(4,10), ylim=c(0,0.7),  prob = T, main='Histogram of DILG and PREP')
d1=density(USJudgeRatings$DILG)
lines(d1, col='red')

hist(USJudgeRatings$PREP, col=rgb(0,0,1,1/4), prob = T, add=T)
d1=density(USJudgeRatings$PREP)
lines(d1, col='green')

legend( 'topright', fill = c(rgb(1,0,0,1/4), rgb(0,0,1,1/4)), legend = c('DILG', 'PREP'))
box()
```
As we expected, the graphs tend to show that two very correlated variables tend to follow similar laws. Here, we can see that the densities of DILG and PREP are extremely similar. As we have only 43 observations, the difference between the two lines might be exagerated because of approximation. 

Knowing this, we can try to find the type of law of only one variable to get an idea of both laws. 

```{r}
# For DILG
par(mfrow=c(1,3))
hist(USJudgeRatings$DILG, prob=TRUE)
d = density(USJudgeRatings$DILG)
lines(d, col='red')

norm_distrib_DILG=rnorm(43,mean(USJudgeRatings$DILG))

hist(norm_distrib_DILG, prob=TRUE)
d = density(norm_distrib_DILG)
lines(d, col='red')

qqnorm(USJudgeRatings$DILG)
qqline(USJudgeRatings$DILG)
```


```{r}
# For INTG
par(mfrow=c(1,3))
hist(USJudgeRatings$INTG, prob=TRUE)
d = density(USJudgeRatings$INTG)
lines(d, col='red')

norm_distrib_INTG=rnorm(43,mean(USJudgeRatings$INTG))

hist(norm_distrib_INTG, prob=TRUE)
d = density(norm_distrib_INTG)
lines(d, col='red')

qqnorm(USJudgeRatings$INTG)
qqline(USJudgeRatings$INTG)
```


```{r}
# For DMNR
par(mfrow=c(1,3))
hist(USJudgeRatings$DMNR, prob=TRUE)
d = density(USJudgeRatings$DMNR)
lines(d, col='red')

norm_distrib_DMNR=rnorm(43,mean(USJudgeRatings$DMNR))

hist(norm_distrib_DMNR, prob=TRUE)
d = density(norm_distrib_DMNR)
lines(d, col='red')

qqnorm(USJudgeRatings$DMNR)
qqline(USJudgeRatings$DMNR)
```


```{r}
# For CFMG
par(mfrow=c(1,3))
hist(USJudgeRatings$CFMG, prob=TRUE)
d = density(USJudgeRatings$CFMG)
lines(d, col='red')

norm_distrib_CFMG=rnorm(43,mean(USJudgeRatings$CFMG))

hist(norm_distrib_CFMG, prob=TRUE)
d = density(norm_distrib_CFMG)
lines(d, col='red')

qqnorm(USJudgeRatings$CFMG)
qqline(USJudgeRatings$CFMG)
```


```{r}
# For DECI
par(mfrow=c(1,3))
hist(USJudgeRatings$DECI, prob=TRUE)
d = density(USJudgeRatings$DECI)
lines(d, col='red')

norm_distrib_DECI=rnorm(43,mean(USJudgeRatings$DECI))

hist(norm_distrib_DECI, prob=TRUE)
d = density(norm_distrib_DECI)
lines(d, col='red')

qqnorm(USJudgeRatings$DECI)
qqline(USJudgeRatings$DECI)
```


```{r}
# For PREP
par(mfrow=c(1,3))
hist(USJudgeRatings$PREP, prob=TRUE)
d = density(USJudgeRatings$PREP)
lines(d, col='red')

norm_distrib_PREP=rnorm(43,mean(USJudgeRatings$PREP))

hist(norm_distrib_PREP, prob=TRUE)
d = density(norm_distrib_PREP)
lines(d, col='red')

qqnorm(USJudgeRatings$PREP)
qqline(USJudgeRatings$PREP)
```


```{r}
# For FAMI
par(mfrow=c(1,3))
hist(USJudgeRatings$FAMI, prob=TRUE)
d = density(USJudgeRatings$FAMI)
lines(d, col='red')

norm_distrib_FAMI=rnorm(43,mean(USJudgeRatings$FAMI))

hist(norm_distrib_FAMI, prob=TRUE)
d = density(norm_distrib_FAMI)
lines(d, col='red')

qqnorm(USJudgeRatings$FAMI)
qqline(USJudgeRatings$FAMI)
```


```{r}
# For ORAL
par(mfrow=c(1,3))
hist(USJudgeRatings$ORAL, prob=TRUE)
d = density(USJudgeRatings$ORAL)
lines(d, col='red')

norm_distrib_ORAL=rnorm(43,mean(USJudgeRatings$ORAL))

hist(norm_distrib_ORAL, prob=TRUE)
d = density(norm_distrib_ORAL)
lines(d, col='red')

qqnorm(USJudgeRatings$ORAL)
qqline(USJudgeRatings$ORAL)
```


```{r}
# For WRIT
par(mfrow=c(1,3))
hist(USJudgeRatings$WRIT, prob=TRUE)
d = density(USJudgeRatings$WRIT)
lines(d, col='red')

norm_distrib_WRIT=rnorm(43,mean(USJudgeRatings$WRIT))

hist(norm_distrib_WRIT, prob=TRUE)
d = density(norm_distrib_WRIT)
lines(d, col='red')

qqnorm(USJudgeRatings$WRIT)
qqline(USJudgeRatings$WRIT)
```


```{r}
# For PHYS
hist(USJudgeRatings$PHYS, prob=TRUE)
d = density(USJudgeRatings$PHYS)
lines(d, col='red')

norm_distrib_PHYS=rnorm(43,mean(USJudgeRatings$PHYS))

hist(norm_distrib_PHYS, prob=TRUE)
d = density(norm_distrib_PHYS)
lines(d, col='red')

qqnorm(USJudgeRatings$PHYS)
qqline(USJudgeRatings$PHYS)
```


```{r}
# For RTEN
par(mfrow=c(1,3))
hist(USJudgeRatings$RTEN, prob=TRUE)
d = density(USJudgeRatings$RTEN)
lines(d, col='red')

norm_distrib_RTEN=rnorm(43,mean(USJudgeRatings$RTEN))

hist(norm_distrib_RTEN, prob=TRUE)
d = density(norm_distrib_RTEN)
lines(d, col='red')

qqnorm(USJudgeRatings$RTEN)
qqline(USJudgeRatings$RTEN)
```





b. Multivariate analysis 

The idea is to compare two variables that look very correlated and two that don't look that correlated.
To have a first vision of what the difference is, we chose to choose one variable, DILG which corresponds to Diligence, and to plot the graph of DILG vs CONT, not very correlated, and DILG vs DECI, which are very correlated. We recall that we got those correlation information thanks to our correlation heatmap.

EST CE QUE LES SCATTER;SMOOTH NE DECALENT PAS LES POINTS ???
```{r}
par(mfrow=c(1,2))

scatter.smooth(USJudgeRatings[,c(1,4)])
title("Plot of CONT vs DILG, not correlated", cex.main=1)

scatter.smooth(USJudgeRatings[,c(6,4)])
title("Plot of DECI vs DILG,very correlated", cex.main=1)
```
\textbf{4. Conclusion}

From this dataset, we can see that the criterion used to evaluate the judges' performance mainly follow normal ditributions, whoch seems quite consistence with common methods of notation. Also, the fact that the grades attributed for each category of expertise are uncorrelated with the first variable is telling us that the laywers are impartial, which is a good thing. 

However, we lack a lot of information to do this study. For instance, we don't know if all judges have been graded by the same lawyer, of if there were many lawyers. In the second case, we don't have the number of lawyers and the information of who graded who. This could have given us more information on the grades and could have explained the variance for instance (if one lawyer was given lawer grades than another one for instance). 

Also, we don't know the purpose of this grade and don't have a lot of context.


